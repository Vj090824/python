{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40bf0fc-4261-4d6f-b18e-8620cb6baa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Objective:\n",
    "The objective of this assignment is to implement PCA on a given dataset and analyse the results.\n",
    "\n",
    "Download the wine dataset from the UCI Machine Learning Repository\n",
    "Load the dataset into a Pandas dataframe.\n",
    "Split the dataset into features and target variables.\n",
    "Perform data preprocessing (e.g., scaling, normalisation, missing value imputation) as necessary.\n",
    "Implement PCA on the preprocessed dataset using the scikit-learn library.\n",
    "Determine the optimal number of principal components to retain based on the explained variance ratio.\n",
    "Visualise the results of PCA using a scatter plot.\n",
    "Perform clustering on the PCA-transformed data using K-Means clustering algorithm.\n",
    "Interpret the results of PCA and clustering analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ans:\n",
    "\n",
    "\n",
    "Certainly, I can provide you with a step-by-step guide to perform PCA and clustering\n",
    "analysis on the wine dataset. Here's how you can do it:\n",
    "\n",
    "**Step 1: Download and Load the Dataset**\n",
    "You can download the wine dataset from the UCI Machine Learning Repository or any other source \n",
    "where it's available. Once downloaded, load the dataset into a Pandas dataframe.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a Pandas dataframe\n",
    "url = \"URL_TO_WINE_DATASET\"\n",
    "column_names = [\"class\", \"Alcohol\", \"Malicacid\", \"Ash\", \"Alcalinity_of_ash\", \"Magnesium\", \"Total_phenols\", \n",
    "                \"Flavanoids\", \"Nonflavanoid_phenols\", \"Proanthocyanins\", \"Color_intensity\", \"Hue\", \n",
    "                \"0D280_0D315_of_diluted_wines\", \"Proline\"]\n",
    "data = pd.read_csv(url, names=column_names)\n",
    "\n",
    "\n",
    "**Step 2: Data Preprocessing**\n",
    "Before applying PCA, you should preprocess the data. This may involve scaling, normalization,\n",
    "and handling any missing values. Ensure that your data is in a suitable format for PCA.\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(columns=['class'])  # Features\n",
    "y = data['class']  # Target variable\n",
    "\n",
    "# Standardize the features (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# You may also need to handle missing values if there are any\n",
    "# You can use methods like imputation with the mean, median, or mode.\n",
    "\n",
    "\n",
    "**Step 3: Implement PCA**\n",
    "Use scikit-learn's PCA module to perform Principal Component Analysis on the preprocessed data.\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define the number of components you want to retain (you'll determine this later)\n",
    "n_components = None  # You can set it to None initially\n",
    "\n",
    "# Create a PCA instance and fit it to your scaled data\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "\n",
    "**Step 4: Determine the Optimal Number of Principal Components**\n",
    "To determine the optimal number of principal components to retain,\n",
    "you can plot the explained variance ratio.\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance.cumsum(), marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.title('Explained Variance vs. Number of Components')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "The above plot will help you decide how many principal components to retain. \n",
    "You can choose a threshold for the explained variance ratio (e.g., 95%) and \n",
    "select the corresponding number of components.\n",
    "\n",
    "**Step 5: Visualize the Results of PCA**\n",
    "You can create a scatter plot to visualize the data in the reduced dimensionality.\n",
    "\n",
    "\n",
    "# Assuming you have selected a number of components (e.g., 2)\n",
    "n_components = 2\n",
    "X_pca = X_pca[:, :n_components]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA Scatter Plot')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "**Step 6: Perform Clustering (K-Means) on PCA-transformed Data**\n",
    "You can perform clustering on the PCA-transformed data using the K-Means clustering algorithm. \n",
    "The number of clusters (k) should be determined based on your problem or\n",
    "through techniques like the elbow method.\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Determine the optimal number of clusters (k) using the elbow method or other techniques\n",
    "k = 3  # You need to decide the appropriate value\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "cluster_labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "\n",
    "**Step 7: Interpret the Results**\n",
    "Interpretation of the results involves analyzing the clusters formed and understanding how the data \n",
    "points are grouped in the reduced dimensionality space. You can evaluate the quality of clusters using \n",
    "various metrics like Silhouette Score or domain-specific knowledge.\n",
    "\n",
    "Remember to adjust the parameters and techniques as needed based on your specific dataset and objectives.\n",
    "Additionally, you can explore other dimensionality reduction techniques\n",
    "and clustering algorithms to compare their performance.\n",
    "\n",
    "\n",
    "\n",
    "Variables Table\n",
    "Variable Name\t         Role\t                    Type\t                 Demographic\t       Description\tUnits\tMissing Values\n",
    "class\t                   Target\t               Categorical\t\t\t     false\n",
    "Alcohol\t                      Feature             Continuous\t\t\t\t false\n",
    "Malicacid                     Feature\t          Continuous\t\t\t\t false\n",
    "Ash                            Feature           Continuous\t\t\t\t     false\n",
    "Alcalinity_of_ash\t           Feature           Continuous\t\t\t         false\n",
    "Magnesium                      Feature          Integer\t\t\t\t         false\n",
    "Total_phenols\t               Feature\t        Continuous\t\t\t\t     false\n",
    "Flavanoids\t                    Feature\t        Continuous\t\t\t\t     false\n",
    "Nonflavanoid_phenols\t         Feature\t    Continuous                   false\n",
    "Proanthocyanins                  Feature\t     Continuous\t\t\t\t     false\n",
    "Color_intensity\t                  Feature        Continuous\t\t\t\t     false\n",
    "Hue\t                               Feature       Continuous\t\t\t\t     false\n",
    "0D280_0D315_of_diluted_wines        Feature\t     Continuous\t                 false\n",
    "Proline                             Feature\t \t\tInteger                  false\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
